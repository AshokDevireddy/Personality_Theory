{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_pdf_to_text(pdf_path, roi):\n",
    "    # Convert PDF pages to images\n",
    "    pages = convert_from_path(pdf_path)\n",
    "\n",
    "    # Extract text from each image using Tesseract OCR\n",
    "    text = \"\"\n",
    "    reader = easyocr.Reader(['en'])  # Initialize once outside the function\n",
    "\n",
    "    for page in pages:\n",
    "        page = preprocess_image(page)\n",
    "\n",
    "        # Create a drawing context\n",
    "        draw = ImageDraw.Draw(page)\n",
    "        \n",
    "        # Dimensions\n",
    "        width, height = page.size\n",
    "        \n",
    "        draw.rectangle([0, 0, roi[0], height], fill=\"white\")\n",
    "        draw.rectangle([roi[2], 0, width, height], fill=\"white\")\n",
    "        draw.rectangle([0, 0, width, roi[1]], fill=\"white\")\n",
    "        draw.rectangle([0, roi[3], width, height], fill=\"white\")\n",
    "\n",
    "        text += easyocr_ocr(page, reader) #pytesseract.image_to_string(page)\n",
    "    return text\n",
    "\n",
    "def easyocr_ocr(image, reader):\n",
    "    # Convert PIL Image to NumPy array\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Perform OCR\n",
    "    result = reader.readtext(image_np)\n",
    "    \n",
    "    # Extract and return the detected text\n",
    "    text = \" \".join([item[1] for item in result])\n",
    "    return text\n",
    "\n",
    "def preprocess_image(image):\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    # Convert PIL Image to NumPy array\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply binary thresholding\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Convert back to PIL Image and return\n",
    "    return Image.fromarray(thresh)\n",
    "\n",
    "# Function to process the extracted text\n",
    "def process_text(text):\n",
    "# Add a space before every capital letter\n",
    "    text_with_spaces = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "    \n",
    "    cleaned_content = re.sub(r'(?<![a-z])[-](?![a-z])|[^a-zA-Z\\s-]', '', text_with_spaces)    \n",
    "    cleaned_words = cleaned_content.split()\n",
    "    lowercase_words = [word for word in cleaned_words if not word.isupper()]\n",
    "    filtered_words = [word for word in lowercase_words if len(word) > 1]\n",
    "    prepositions = {\"of\", \"and\", \"or\"}\n",
    "    filtered_words = [word for word in filtered_words if word not in prepositions]\n",
    "\n",
    "    start_index = filtered_words.index(\"abandoned\")\n",
    "    words_from_abandoned = filtered_words[start_index:]\n",
    "    final_words = [word for word in words_from_abandoned if word.islower()]\n",
    "\n",
    "    return final_words\n",
    "\n",
    "def embeddings(words):\n",
    "    word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "    # Generate embeddings for the unique words\n",
    "    embeddings = {word: word_vectors[word] for word in words if word in word_vectors}\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "pdf_text = ocr_pdf_to_text(\"trait_name_a_psycho-lexical_study_removed.pdf\",  (0, 0, 312, 1656))\n",
    "\n",
    "\n",
    "with open('output_text_file', 'w', encoding='utf-8') as file:\n",
    "    file.write(pdf_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4249\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text\n",
    "processed_words = process_text(pdf_text)\n",
    "print(len(processed_words))\n",
    "# print(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n"
     ]
    }
   ],
   "source": [
    "def find_synonyms(word_list):\n",
    "    synonym_groups = []\n",
    "    word_set = set(word_list)  # Convert to set for faster lookup\n",
    "    \n",
    "    for word in word_list:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                # Only add synonyms that are in the original word list\n",
    "                if lemma.name() in word_set:\n",
    "                    synonyms.add(lemma.name())\n",
    "        \n",
    "        if synonyms:\n",
    "            synonym_groups.append(list(synonyms))\n",
    "    \n",
    "    return synonym_groups\n",
    "\n",
    "def merge_overlapping_groups(groups):\n",
    "    merged_groups = []\n",
    "    for group in groups:\n",
    "        # Check if the new group overlaps with an existing group\n",
    "        for merged in merged_groups:\n",
    "            if set(group) & set(merged):  # Check for overlapping words\n",
    "                merged.extend(group)  # Merge groups\n",
    "                merged = list(set(merged))  # Remove duplicates\n",
    "                break\n",
    "        else:\n",
    "            # If no overlapping group is found, add it as a new group\n",
    "            merged_groups.append(group)\n",
    "    return merged_groups\n",
    "\n",
    "initial_synonym_groups = find_synonyms(processed_words)\n",
    "merged_synonym_groups = merge_overlapping_groups(initial_synonym_groups)\n",
    "\n",
    "print(len(merged_synonym_groups))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_embeddings = embeddings(processed_words)\n",
    "\n",
    "# Convert the embeddings into a matrix format\n",
    "matrix_embeddings = np.array(list(word_embeddings.values()))\n",
    "\n",
    "# Max number of clusters to test\n",
    "max_k = 1000\n",
    "inertia = []  \n",
    "\n",
    "# K-Means clustering and evaluating loss\n",
    "for k in range(1, max_k+1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(matrix_embeddings)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "# Plotting number of clusters vs loss\n",
    "plt.plot(range(1, max_k+1), inertia, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clusters_to_embeddings(cluster, embedding_dict):\n",
    "    # Initialize a zero vector for words without an embedding\n",
    "    zero_vector = np.zeros_like(next(iter(embedding_dict.values())))\n",
    "    \n",
    "    # Convert words to embeddings, using a zero vector if the word has no embedding\n",
    "    return [\n",
    "        [embedding_dict.get(word, zero_vector) for word in group] \n",
    "        for group in cluster\n",
    "    ]\n",
    "\n",
    "# Get unique words from the cluster\n",
    "# Generate embeddings for the unique words\n",
    "\n",
    "# Convert words in the cluster to embeddings\n",
    "cluster1_embeddings = convert_clusters_to_embeddings(merged_synonym_groups, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AshokDevireddy/Developer/BSG/myenv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans_chosen_k = KMeans(n_clusters=1176, random_state=42).fit(matrix_embeddings)\n",
    "labels = kmeans_chosen_k.labels_\n",
    "\n",
    "clusters = {i: [] for i in range(1176)}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters[label].append(matrix_embeddings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing wordnet and kmeans clustering\n",
    "\n",
    "\n",
    "\n",
    "def calculate_centroid(embeddings):\n",
    "    \"\"\"Calculate the centroid of a set of embeddings, handling empty lists.\"\"\"\n",
    "    if len(embeddings) == 0:\n",
    "        print(\"Warning: empty embeddings list encountered.\")\n",
    "        return np.zeros((300,))  # Assuming embedding size is 300\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def calculate_group_centroids(cluster):\n",
    "    \"\"\"Calculate the centroid of each group of embeddings within a cluster.\"\"\"\n",
    "    group_centroids = []\n",
    "    for i, group in enumerate(cluster):\n",
    "        centroid = calculate_centroid(group)\n",
    "        if np.all(centroid == 0):  # Check if centroid is a zero vector\n",
    "            print(f\"Warning: Group {i} resulted in a zero-vector centroid.\")\n",
    "        group_centroids.append(centroid)\n",
    "    return group_centroids\n",
    "\n",
    "def calculate_similarity(cluster1, cluster2):\n",
    "    \"\"\"Calculate the cosine similarity between two clusters of embedding groups.\"\"\"\n",
    "    # Calculate the centroids of the embedding groups\n",
    "    centroids1 = calculate_group_centroids(cluster1)\n",
    "    centroids2 = calculate_group_centroids(cluster2)\n",
    "    \n",
    "    # Calculate the overall centroid for each cluster\n",
    "    overall_centroid1 = calculate_centroid(centroids1)\n",
    "    overall_centroid2 = calculate_centroid(centroids2)\n",
    "    \n",
    "    # Calculate and return the cosine similarity between the two overall centroids\n",
    "    return cosine_similarity([overall_centroid1], [overall_centroid2])[0, 0]\n",
    "\n",
    "cluster_wordnet = {i: embedding for i, embedding in enumerate(cluster1_embeddings)}\n",
    "cluster_kmeans = clusters\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "for i in range(1176):\n",
    "    count1 += len(cluster_wordnet[i])\n",
    "    count2 += len(cluster_kmeans[i])\n",
    "print(count1, count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "1575\n"
     ]
    }
   ],
   "source": [
    "print(type(cluster_wordnet[0][0][0]))\n",
    "print(type(cluster_kmeans[0][0][0]))\n",
    "print(len(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 87.1kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.97MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 8.48MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.33MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [01:03<00:00, 6.93MB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# K-Means clustering and evaluating loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(matrix_embeddings)\n\u001b[1;32m     42\u001b[0m     inertia\u001b[38;5;241m.\u001b[39mappend(kmeans\u001b[38;5;241m.\u001b[39minertia_)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Plotting number of clusters vs loss\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "\n",
    "\n",
    "def bert_embeddings(words):\n",
    "    # Load pre-trained model/tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Ensure model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        # Tokenize input word and return as PyTorch tensor\n",
    "        inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Use the embeddings from the last hidden-state\n",
    "        # (size: batch_size, sequence_length, model_hidden_dimension)\n",
    "        embeddings[word] = outputs.last_hidden_state[0, 0].numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "word_embeddings = bert_embeddings(processed_words)\n",
    "\n",
    "# Convert the embeddings into a matrix format\n",
    "matrix_embeddings = np.array(list(word_embeddings.values()))\n",
    "\n",
    "# Max number of clusters to test\n",
    "max_k = 1000\n",
    "inertia = []  \n",
    "\n",
    "# K-Means clustering and evaluating loss\n",
    "for k in range(1, max_k+1, 20):\n",
    "    print(k)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(matrix_embeddings)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "# Plotting number of clusters vs loss\n",
    "plt.plot(range(1, max_k+1), inertia, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "353c9ab84f88cdc71af518963662795b12c8138a7d7168d6e66c982255c21a3f"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
